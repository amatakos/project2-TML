{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a8edb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2733a1528d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import opacus\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from opacus.utils.uniform_sampler import UniformWithReplacementSampler\n",
    "\n",
    "torch.manual_seed(472368)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69d79b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x of shape [B, 1, 28, 28]\n",
    "        x = F.relu(self.conv1(x))  # -> [B, 16, 14, 14]\n",
    "        x = F.max_pool2d(x, 2, 1)  # -> [B, 16, 13, 13]\n",
    "        x = F.relu(self.conv2(x))  # -> [B, 32, 5, 5]\n",
    "        x = F.max_pool2d(x, 2, 1)  # -> [B, 32, 4, 4]\n",
    "        x = x.view(-1, 32 * 4 * 4)  # -> [B, 512]\n",
    "        x = F.relu(self.fc1(x))  # -> [B, 32]\n",
    "        x = self.fc2(x)  # -> [B, 10]\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_step(model, batch):\n",
    "        x, y = batch\n",
    "        # x, y = x.type(torch.LongTensor), y.type(torch.LongTensor)\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        return loss\n",
    "\n",
    "    @staticmethod\n",
    "    def test_step(model, batch):\n",
    "        x, y = batch\n",
    "        # x, y = x.type(torch.LongTensor), y.type(torch.LongTensor)\n",
    "        out = model(x)\n",
    "        loss = model.loss(out, y)\n",
    "        pred = out.argmax(dim=1, keepdim=True)#.type(torch.LongTensor)\n",
    "        corrects = pred.eq(y.view_as(pred)).sum().item()\n",
    "        #preds = out > 0 # Predict y = 1 if P(y = 1) > 0.5\n",
    "        #corrects = torch.tensor(torch.sum(preds == y).item())\n",
    "        return loss, corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89279fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, opt_func, learning_rate, num_epochs, noise_multiplier, clip_bound, delta, verbose=False, random_seed=474237):\n",
    "    optimizer = opt_func(model.parameters(), learning_rate)\n",
    "    privacy_engine = opacus.PrivacyEngine(\n",
    "        accountant=\"rdp\", # Use RDP-based accounting\n",
    "        secure_mode=False, # Should be set to True for production use\n",
    "    )\n",
    "    rng = torch.Generator()\n",
    "    rng.manual_seed(int(random_seed))\n",
    "    model_type = type(model)\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=noise_multiplier,\n",
    "        max_grad_norm=clip_bound,\n",
    "        noise_generator=rng,\n",
    "        loss_reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        for batch in train_loader:\n",
    "            loss = model_type.train_step(model, batch)\n",
    "            loss.backward()\n",
    "            losses.append(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Epoch {}, loss = {}\".format(epoch + 1, torch.sum(loss) / len(train_loader)))\n",
    "\n",
    "    # epsilon, alpha = optimizer.privacy_engine.get_privacy_spent(delta)\n",
    "    epsilon = privacy_engine.get_epsilon(delta)\n",
    "    return epsilon\n",
    "\n",
    "def test(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        total_size = 0\n",
    "        for batch in test_loader:\n",
    "            total_size += len(batch[1])\n",
    "            loss, corrects = model.test_step(model, batch)\n",
    "            losses.append(loss)\n",
    "            accuracies.append(corrects)\n",
    "\n",
    "        average_loss = np.array(loss).sum() / total_size\n",
    "        total_accuracy = np.array(accuracies).sum() / total_size\n",
    "        return average_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c28aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_dataset(train_size=30000):\n",
    "    # Precomputed characteristics of the MNIST dataset\n",
    "    MNIST_MEAN = 0.1307\n",
    "    MNIST_STD = 0.3081\n",
    "    \n",
    "    train_full_dataset = datasets.MNIST(\n",
    "        \"mnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((MNIST_MEAN,), (MNIST_STD,)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    train_subsample_loader = DataLoader(\n",
    "        train_full_dataset, train_size,\n",
    "        generator=torch.Generator().manual_seed(123456789)\n",
    "    )\n",
    "    train_subsample = train_subsample_loader.__iter__().__next__()\n",
    "    train_dataset = TensorDataset(train_subsample[0], train_subsample[1])\n",
    "\n",
    "    test_dataset = datasets.MNIST(\n",
    "        \"mnist\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((MNIST_MEAN,), (MNIST_STD,)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def create_data_loaders(train_tensor, test_tensor, sample_rate, random_seed=4732842):\n",
    "    train_loader = DataLoader(\n",
    "        train_tensor,\n",
    "        batch_size=int(len(train_tensor) * sample_rate),\n",
    "        generator=torch.Generator().manual_seed(random_seed)\n",
    "    )\n",
    "    test_loader = DataLoader(test_tensor, 64)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f62f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(lr, s, C, sr, ne, train_tensor=None, test_tensor=None, verbose=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that executes the whole pipeline from data download to training and running the model.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    lr: learning rate\n",
    "    C: clipping bound\n",
    "    s: noise multiplier\n",
    "    sr: sampling rate\n",
    "    ne: number of epochs\n",
    "    data: the data, if not provided then function downloads the dataset and converts it to torch tensors.\n",
    "    verbose: verbosity parameter\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    eps: epsilon\n",
    "    delta: delta\n",
    "    avg_loss: average loss\n",
    "    acc_total: total accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    if train_tensor is None:\n",
    "        train_tensor, test_tensor = get_mnist_dataset() # Note that this function is modified to provide the adult dataset\n",
    "    \n",
    "    delta = 1e-5\n",
    "\n",
    "    input_dim = train_tensor[0][0].size(dim=0)\n",
    "    train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sr) # recreate loaders with different sampling rate\n",
    "\n",
    "    model = SampleConvNet()\n",
    "    epsilon = train(\n",
    "        model, train_loader, torch.optim.SGD, lr, ne,\n",
    "        s, C, delta, verbose=verbose\n",
    "    )\n",
    "    \n",
    "    average_loss, total_accuracy = test(model, test_loader)\n",
    "    print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))\n",
    "    \n",
    "    return epsilon, delta, average_loss, total_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a08c0a",
   "metadata": {},
   "source": [
    "### i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125668a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "train_tensor, test_tensor = get_mnist_dataset()\n",
    "\n",
    "# Hyperparameters\n",
    "delta = 1e-5\n",
    "\n",
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 1.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.004\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e8c2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    for x in batch:\n",
    "        print(x.size())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae7bd8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\torch\\lib\\site-packages\\opacus\\privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.002683916362002492\n",
      "Epoch 2, loss = 0.0017444395925849676\n",
      "Epoch 3, loss = 0.003123308066278696\n",
      "Epoch 4, loss = 0.0035235031973570585\n",
      "Epoch 5, loss = 0.005509137641638517\n",
      "Epsilon: 0.6188836247099821, Delta: 1e-05\n",
      "Loss: 2.761020790785551e-07, Accuracy: 0.8917\n"
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "train_tensor, test_tensor = get_mnist_dataset()\n",
    "\n",
    "# Hyperparameters\n",
    "delta = 1e-5\n",
    "\n",
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 1.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.004\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a872f",
   "metadata": {},
   "source": [
    "### Accuracy on test: 89.17\\% with $\\epsilon = 0.619$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9278f3",
   "metadata": {},
   "source": [
    "### ii) \n",
    "\n",
    "For this task, I followed a similar approach to before with the staircase optimization method keeping some parameters fixed and fine-tuning the rest, but the tests were done in an unorganized manner, trying to figure out each time towards what direction to move in the parameter space. I report two solutions I found, one focusing on accuracy and the other on $\\epsilon$. If I had to choose one I would choose the solution focusing on lower $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fb9b2",
   "metadata": {},
   "source": [
    "### Use full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5640f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "train_tensor, test_tensor = get_mnist_dataset(train_size=60000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae4bddf",
   "metadata": {},
   "source": [
    "### Solution focusing more on $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "254bcf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0011947514722123742\n",
      "Epoch 2, loss = 0.0009345292346552014\n",
      "Epoch 3, loss = 0.0014193110400810838\n",
      "Epsilon: 0.34999697730814633, Delta: 1e-05\n",
      "Loss: 2.0841059740632773e-07, Accuracy: 0.9095\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.3\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.4\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "\n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0664c71",
   "metadata": {},
   "source": [
    "### Solution focusing more on accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a69e6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.001240633544512093\n",
      "Epoch 2, loss = 0.0009662366937845945\n",
      "Epoch 3, loss = 0.0007573306211270392\n",
      "Epoch 4, loss = 0.0007817731820978224\n",
      "Epoch 5, loss = 0.0013070753775537014\n",
      "Epoch 6, loss = 0.0009868830675259233\n",
      "Epoch 7, loss = 0.0006644311361014843\n",
      "Epoch 8, loss = 0.0005835895426571369\n",
      "Epoch 9, loss = 0.0004939452628605068\n",
      "Epoch 10, loss = 0.0001406442024745047\n",
      "Epoch 11, loss = 0.0005175182595849037\n",
      "Epoch 12, loss = 0.00027791669708676636\n",
      "Epoch 13, loss = 0.00043873387039639056\n",
      "Epoch 14, loss = 0.00025171658489853144\n",
      "Epoch 15, loss = 0.00016195156786125153\n",
      "Epoch 16, loss = 0.0013775202678516507\n",
      "Epoch 17, loss = 0.0013931195717304945\n",
      "Epoch 18, loss = 0.0005723065696656704\n",
      "Epoch 19, loss = 0.0006628859555348754\n",
      "Epoch 20, loss = 0.00030533759854733944\n",
      "Epsilon: 1.0649553579562177, Delta: 1e-05\n",
      "Loss: 6.340327217913e-10, Accuracy: 0.9506\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.1\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "\n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363f307e",
   "metadata": {},
   "source": [
    "### Below are the various runs I did to end up with the above solutions. Feel free to take a look but it's quite messy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f1c1f17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.004592562559992075\n",
      "Epoch 2, loss = 0.004874060861766338\n",
      "Epoch 3, loss = 0.002444712445139885\n",
      "Epoch 4, loss = 0.0037888793740421534\n",
      "Epoch 5, loss = 0.002816338324919343\n",
      "Epoch 6, loss = 0.004204682074487209\n",
      "Epoch 7, loss = 0.0016670236364006996\n",
      "Epoch 8, loss = 0.0017426135018467903\n",
      "Epoch 9, loss = 0.00267465109936893\n",
      "Epoch 10, loss = 0.003986026626080275\n",
      "Epsilon: 14.148936019106882, Delta: 1e-05\n",
      "Loss: 4.6494631096720696e-07, Accuracy: 0.9482\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 0.5\n",
    "# Clipping norm\n",
    "clip_bound = 1.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50b8082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.006581319961696863\n",
      "Epoch 2, loss = 0.004990887828171253\n",
      "Epoch 3, loss = 0.0025741623248904943\n",
      "Epoch 4, loss = 0.003746436443179846\n",
      "Epoch 5, loss = 0.002718033967539668\n",
      "Epoch 6, loss = 0.00485917879268527\n",
      "Epoch 7, loss = 0.002202379982918501\n",
      "Epoch 8, loss = 0.0020739282481372356\n",
      "Epoch 9, loss = 0.003464684821665287\n",
      "Epoch 10, loss = 0.0034636142663657665\n",
      "Epsilon: 14.148936019106882, Delta: 1e-05\n",
      "Loss: 2.892867451009806e-09, Accuracy: 0.9516\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.5\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 0.5\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46c77121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.02185060642659664\n",
      "Epoch 2, loss = 0.05237944424152374\n",
      "Epoch 3, loss = 0.048090822994709015\n",
      "Epoch 4, loss = 0.02320609986782074\n",
      "Epoch 5, loss = 0.02974412962794304\n",
      "Epoch 6, loss = 0.0327458418905735\n",
      "Epoch 7, loss = 0.03187898173928261\n",
      "Epoch 8, loss = 0.023978877812623978\n",
      "Epoch 9, loss = 0.0343267060816288\n",
      "Epoch 10, loss = 0.03831883892416954\n",
      "Epsilon: 0.20782626385054587, Delta: 1e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\torch\\lib\\site-packages\\opacus\\accountants\\analysis\\rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0001930263638496399, Accuracy: 0.2496\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.5\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 5.\n",
    "# Clipping norm\n",
    "clip_bound = 1.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303ed611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.005282752215862274\n",
      "Epoch 2, loss = 0.006581668742001057\n",
      "Epoch 3, loss = 0.0058989133685827255\n",
      "Epoch 4, loss = 0.007384671363979578\n",
      "Epoch 5, loss = 0.010155069641768932\n",
      "Epoch 6, loss = 0.016776299104094505\n",
      "Epoch 7, loss = 0.01580929011106491\n",
      "Epoch 8, loss = 0.013244871981441975\n",
      "Epoch 9, loss = 0.014813763089478016\n",
      "Epoch 10, loss = 0.016258051618933678\n",
      "Epsilon: 0.6054551619620246, Delta: 1e-05\n",
      "Loss: 0.0001400646924972534, Accuracy: 0.8585\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.5\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 2.\n",
    "# Clipping norm\n",
    "clip_bound = 1.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "becf9707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.005998621694743633\n",
      "Epoch 2, loss = 0.006012042053043842\n",
      "Epoch 3, loss = 0.004418761469423771\n",
      "Epoch 4, loss = 0.005905027035623789\n",
      "Epoch 5, loss = 0.006454506888985634\n",
      "Epoch 6, loss = 0.0077430484816432\n",
      "Epoch 7, loss = 0.006105558946728706\n",
      "Epoch 8, loss = 0.005686644464731216\n",
      "Epoch 9, loss = 0.00818079337477684\n",
      "Epoch 10, loss = 0.011749284341931343\n",
      "Epoch 11, loss = 0.01217640656977892\n",
      "Epoch 12, loss = 0.011672627180814743\n",
      "Epoch 13, loss = 0.01663767732679844\n",
      "Epoch 14, loss = 0.01435843389481306\n",
      "Epoch 15, loss = 0.012777172029018402\n",
      "Epoch 16, loss = 0.013146179728209972\n",
      "Epoch 17, loss = 0.013701868243515491\n",
      "Epoch 18, loss = 0.01678811013698578\n",
      "Epoch 19, loss = 0.01779244840145111\n",
      "Epoch 20, loss = 0.017634298652410507\n",
      "Epsilon: 0.5358015231964641, Delta: 1e-05\n",
      "Loss: 0.00011764787435531616, Accuracy: 0.8553\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.3\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 3.\n",
    "# Clipping norm\n",
    "clip_bound = 1.\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40fd68a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.00932625774294138\n",
      "Epoch 2, loss = 0.0064944312907755375\n",
      "Epoch 3, loss = 0.0036767662968486547\n",
      "Epoch 4, loss = 0.004332357551902533\n",
      "Epoch 5, loss = 0.002935884054750204\n",
      "Epoch 6, loss = 0.005231067538261414\n",
      "Epoch 7, loss = 0.0031102898065000772\n",
      "Epoch 8, loss = 0.003300727577880025\n",
      "Epoch 9, loss = 0.0051866318099200726\n",
      "Epoch 10, loss = 0.00507062254473567\n",
      "Epsilon: 0.8136647699145032, Delta: 1e-05\n",
      "Loss: 3.0039425939321517e-06, Accuracy: 0.9188\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.2\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.6\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6d821f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.009068525396287441\n",
      "Epoch 2, loss = 0.007501861546188593\n",
      "Epoch 3, loss = 0.0038892205338925123\n",
      "Epoch 4, loss = 0.005079340189695358\n",
      "Epoch 5, loss = 0.0033665813971310854\n",
      "Epsilon: 1.4824070580033397, Delta: 1e-05\n",
      "Loss: 1.533075124025345e-05, Accuracy: 0.8767\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed76b02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.006191704887896776\n",
      "Epoch 2, loss = 0.006120648700743914\n",
      "Epoch 3, loss = 0.003174602286890149\n",
      "Epoch 4, loss = 0.004008895251899958\n",
      "Epoch 5, loss = 0.0032801807392388582\n",
      "Epoch 6, loss = 0.005756950471550226\n",
      "Epoch 7, loss = 0.0034871576353907585\n",
      "Epoch 8, loss = 0.0033769498113542795\n",
      "Epoch 9, loss = 0.004921657498925924\n",
      "Epoch 10, loss = 0.004393521696329117\n",
      "Epsilon: 0.8916069841308477, Delta: 1e-05\n",
      "Loss: 3.9145979098975656e-08, Accuracy: 0.9331\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.4\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.5\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af99adc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.003084031865000725\n",
      "Epoch 2, loss = 0.0017990535125136375\n",
      "Epoch 3, loss = 0.002012252574786544\n",
      "Epoch 4, loss = 0.003299159463495016\n",
      "Epoch 5, loss = 0.003086068667471409\n",
      "Epsilon: 0.6188836247099821, Delta: 1e-05\n",
      "Loss: 4.5920062810182574e-06, Accuracy: 0.9249\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.004\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "613a4b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.003079631133005023\n",
      "Epoch 2, loss = 0.0013738359557464719\n",
      "Epoch 3, loss = 0.0028438176959753036\n",
      "Epoch 4, loss = 0.004077156540006399\n",
      "Epoch 5, loss = 0.004997132811695337\n",
      "Epsilon: 0.6188836247099821, Delta: 1e-05\n",
      "Loss: 8.957058191299438e-07, Accuracy: 0.8984\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.4\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.004\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8fc2edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.007676322013139725\n",
      "Epoch 2, loss = 0.006448304280638695\n",
      "Epoch 3, loss = 0.0046762097626924515\n",
      "Epoch 4, loss = 0.003859391203150153\n",
      "Epoch 5, loss = 0.0033902975264936686\n",
      "Epsilon: 0.8361046634978282, Delta: 1e-05\n",
      "Loss: 3.926738351583481e-06, Accuracy: 0.878\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.008\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "525d4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0019237279193475842\n",
      "Epoch 2, loss = 0.002529165707528591\n",
      "Epoch 3, loss = 0.002950016176328063\n",
      "Epoch 4, loss = 0.008137031458318233\n",
      "Epoch 5, loss = 0.00857428926974535\n",
      "Epsilon: 0.48191032106995346, Delta: 1e-05\n",
      "Loss: 0.0001335606575012207, Accuracy: 0.8042\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.002\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb55182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0014880017843097448\n",
      "Epoch 2, loss = 0.0015044690808281302\n",
      "Epoch 3, loss = 0.0012882747687399387\n",
      "Epoch 4, loss = 0.00246347370557487\n",
      "Epoch 5, loss = 0.0035814139991998672\n",
      "Epsilon: 0.5539421596678942, Delta: 1e-05\n",
      "Loss: 4.2136919498443604e-05, Accuracy: 0.8893\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.003\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "267877bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0032873814925551414\n",
      "Epoch 2, loss = 0.002078196732327342\n",
      "Epoch 3, loss = 0.0013927429681643844\n",
      "Epoch 4, loss = 0.0016172376926988363\n",
      "Epoch 5, loss = 0.003160846885293722\n",
      "Epoch 6, loss = 0.0018871542997658253\n",
      "Epoch 7, loss = 0.001472540432587266\n",
      "Epoch 8, loss = 0.001204586704261601\n",
      "Epoch 9, loss = 0.001743166008964181\n",
      "Epoch 10, loss = 0.000683935359120369\n",
      "Epsilon: 0.6793064502497669, Delta: 1e-05\n",
      "Loss: 1.5623224899172782e-06, Accuracy: 0.9169\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.125\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.003\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "577c72dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.005910482723265886\n",
      "Epoch 2, loss = 0.004320061299949884\n",
      "Epoch 3, loss = 0.0026985779404640198\n",
      "Epoch 4, loss = 0.0020844144746661186\n",
      "Epoch 5, loss = 0.0029985825531184673\n",
      "Epoch 6, loss = 0.0020999787375330925\n",
      "Epoch 7, loss = 0.001447996823117137\n",
      "Epoch 8, loss = 0.0012396490201354027\n",
      "Epoch 9, loss = 0.0014371193246915936\n",
      "Epoch 10, loss = 0.001125303446315229\n",
      "Epsilon: 0.6793064502497669, Delta: 1e-05\n",
      "Loss: 1.5159940719604492e-05, Accuracy: 0.8572\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.05\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.003\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f992879",
   "metadata": {},
   "source": [
    "# Subsample train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "cdd59748",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor, test_tensor = get_mnist_dataset(train_size=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ec750e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0072714658454060555\n",
      "Epoch 2, loss = 0.005599441006779671\n",
      "Epoch 3, loss = 0.004659594036638737\n",
      "Epoch 4, loss = 0.0025474040303379297\n",
      "Epoch 5, loss = 0.004874274600297213\n",
      "Epoch 6, loss = 0.006577667314559221\n",
      "Epoch 7, loss = 0.007730745244771242\n",
      "Epoch 8, loss = 0.012412753887474537\n",
      "Epoch 9, loss = 0.004126830026507378\n",
      "Epoch 10, loss = 0.00927735585719347\n",
      "Epoch 11, loss = 0.0016702455468475819\n",
      "Epoch 12, loss = 0.01121425349265337\n",
      "Epoch 13, loss = 0.00893873255699873\n",
      "Epoch 14, loss = 0.004142715595662594\n",
      "Epoch 15, loss = 0.018805280327796936\n",
      "Epoch 16, loss = 0.01783066615462303\n",
      "Epoch 17, loss = 0.01347443275153637\n",
      "Epoch 18, loss = 0.015416118316352367\n",
      "Epoch 19, loss = 0.021014664322137833\n",
      "Epoch 20, loss = 0.011776248924434185\n",
      "Epsilon: 1.0672803504742279, Delta: 1e-05\n",
      "Loss: 0.00025326640605926513, Accuracy: 0.5656\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.02\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 1.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.004\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21411c45",
   "metadata": {},
   "source": [
    "### Take full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e86c83dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor, test_tensor = get_mnist_dataset(train_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a7085b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0014719408936798573\n",
      "Epoch 2, loss = 0.0017010967712849379\n",
      "Epoch 3, loss = 0.0018709610449150205\n",
      "Epoch 4, loss = 0.0009801796404644847\n",
      "Epoch 5, loss = 0.0011247111251577735\n",
      "Epsilon: 0.5539421596678942, Delta: 1e-05\n",
      "Loss: 8.326234295964241e-07, Accuracy: 0.9311\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.003\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "918df4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0010992912575602531\n",
      "Epoch 2, loss = 0.0010812802938744426\n",
      "Epoch 3, loss = 0.000982529716566205\n",
      "Epoch 4, loss = 0.0005915345391258597\n",
      "Epoch 5, loss = 0.002678802004083991\n",
      "Epsilon: 0.38015269074646874, Delta: 1e-05\n",
      "Loss: 2.2683321731165052e-09, Accuracy: 0.9165\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.4\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0e62e680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0011292078997939825\n",
      "Epoch 2, loss = 0.0009494057740084827\n",
      "Epoch 3, loss = 0.0009113266132771969\n",
      "Epsilon: 0.34999697730814633, Delta: 1e-05\n",
      "Loss: 6.230343133211136e-08, Accuracy: 0.9064\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.3\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.4\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 3\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "e232d617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0010796352289617062\n",
      "Epoch 2, loss = 0.0009286236600019038\n",
      "Epsilon: 0.29078942973607796, Delta: 1e-05\n",
      "Loss: 1.4755974523723125e-06, Accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.5\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 2\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "610ec274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0019237279193475842\n",
      "Epoch 2, loss = 0.002529165707528591\n",
      "Epoch 3, loss = 0.002950016176328063\n",
      "Epoch 4, loss = 0.008137031458318233\n",
      "Epoch 5, loss = 0.00857428926974535\n",
      "Epsilon: 0.48191032106995346, Delta: 1e-05\n",
      "Loss: 0.0001335606575012207, Accuracy: 0.8042\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.3\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.002\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "805838b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\envs\\torch\\lib\\site-packages\\opacus\\privacy_engine.py:133: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0006509917438961565\n",
      "Epoch 2, loss = 0.0010080678621307015\n",
      "Epoch 3, loss = 0.0015376530354842544\n",
      "Epoch 4, loss = 0.0006745706195943058\n",
      "Epoch 5, loss = 0.0005489647737704217\n",
      "Epsilon: 0.7266407511899576, Delta: 1e-05\n",
      "Loss: 1.3921691477298737e-05, Accuracy: 0.9079\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.001\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bcf86c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0011801021173596382\n",
      "Epoch 2, loss = 0.00151817814912647\n",
      "Epoch 3, loss = 0.003622174495831132\n",
      "Epoch 4, loss = 0.003054977161809802\n",
      "Epoch 5, loss = 0.0022164147812873125\n",
      "Epsilon: 0.288758623479293, Delta: 1e-05\n",
      "Loss: 0.00019924048185348511, Accuracy: 0.8136\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.5\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.001\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "71400e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0003449567884672433\n",
      "Epoch 2, loss = 0.0012119680177420378\n",
      "Epoch 3, loss = 0.002123486017808318\n",
      "Epoch 4, loss = 0.0013365363702178001\n",
      "Epoch 5, loss = 0.0005887767183594406\n",
      "Epsilon: 0.47182569674103964, Delta: 1e-05\n",
      "Loss: 9.161260724067688e-05, Accuracy: 0.8789\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.25\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.2\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.001\n",
    "# Number of epochs\n",
    "num_epochs = 5\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b791d",
   "metadata": {},
   "source": [
    "### Solutions focusing on accuracy rather than epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cc597a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0010874260915443301\n",
      "Epoch 2, loss = 0.001134716090746224\n",
      "Epoch 3, loss = 0.001119724242016673\n",
      "Epoch 4, loss = 0.0007937122718431056\n",
      "Epoch 5, loss = 0.0014535702066496015\n",
      "Epoch 6, loss = 0.000990800792351365\n",
      "Epoch 7, loss = 0.001129625248722732\n",
      "Epoch 8, loss = 0.0005048282910138369\n",
      "Epoch 9, loss = 0.0006879009306430817\n",
      "Epoch 10, loss = 0.0005330557469278574\n",
      "Epoch 11, loss = 0.0002691051922738552\n",
      "Epoch 12, loss = 0.0006228769198060036\n",
      "Epoch 13, loss = 0.00038238742854446173\n",
      "Epoch 14, loss = 0.00036568782525137067\n",
      "Epoch 15, loss = 0.00024392975319642574\n",
      "Epoch 16, loss = 0.0009237586054950953\n",
      "Epoch 17, loss = 0.0010624260175973177\n",
      "Epoch 18, loss = 0.00045225402573123574\n",
      "Epoch 19, loss = 0.00042965877219103277\n",
      "Epoch 20, loss = 0.00043918381561525166\n",
      "Epsilon: 1.0649553579562177, Delta: 1e-05\n",
      "Loss: 8.389782742597162e-08, Accuracy: 0.9465\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.1\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.\n",
    "# Clipping norm\n",
    "clip_bound = 0.5\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 20\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d9e75a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0010490822605788708\n",
      "Epoch 2, loss = 0.000889830756932497\n",
      "Epoch 3, loss = 0.0007502700318582356\n",
      "Epoch 4, loss = 0.000569394207559526\n",
      "Epoch 5, loss = 0.0013765111798420548\n",
      "Epoch 6, loss = 0.0007615330978296697\n",
      "Epoch 7, loss = 0.0005313268047757447\n",
      "Epoch 8, loss = 0.0004097585624549538\n",
      "Epoch 9, loss = 0.000529853452462703\n",
      "Epoch 10, loss = 0.00012683945533353835\n",
      "Epoch 11, loss = 0.00021222926443442702\n",
      "Epoch 12, loss = 0.0007154532941058278\n",
      "Epoch 13, loss = 0.00044153182534500957\n",
      "Epoch 14, loss = 0.0005186291527934372\n",
      "Epoch 15, loss = 0.0002556252002250403\n",
      "Epoch 16, loss = 0.000923754763789475\n",
      "Epoch 17, loss = 0.0014770939014852047\n",
      "Epoch 18, loss = 0.0004877931496594101\n",
      "Epoch 19, loss = 0.0005806299741379917\n",
      "Epoch 20, loss = 0.0005085001466795802\n",
      "Epoch 21, loss = 0.0012865723110735416\n",
      "Epoch 22, loss = 0.002021565567702055\n",
      "Epoch 23, loss = 0.00034536575549282134\n",
      "Epoch 24, loss = 0.00017896539065986872\n",
      "Epoch 25, loss = 0.0006112336413934827\n",
      "Epoch 26, loss = 0.0015038525452837348\n",
      "Epoch 27, loss = 0.00022989482386037707\n",
      "Epoch 28, loss = 0.00014759799523744732\n",
      "Epoch 29, loss = 0.0008587486809119582\n",
      "Epoch 30, loss = 0.00024803547421470284\n",
      "Epsilon: 1.2305742338299492, Delta: 1e-05\n",
      "Loss: 1.7652781680226326e-07, Accuracy: 0.9383\n"
     ]
    }
   ],
   "source": [
    "# Learning rate for training\n",
    "learning_rate = 0.1\n",
    "# Ratio of the standard deviation to the clipping norm\n",
    "noise_multiplier = 1.\n",
    "# Clipping norm\n",
    "clip_bound = 0.8\n",
    "# Batch size as a fraction of full data size\n",
    "sample_rate = 0.0015\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "    \n",
    "train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "\n",
    "model = SampleConvNet()\n",
    "epsilon = train(\n",
    "    model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "    noise_multiplier, clip_bound, delta, verbose=True\n",
    ")\n",
    "print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "average_loss, total_accuracy = test(model, test_loader)\n",
    "print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8e03c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss = 0.0033979681320488453\n",
      "Epsilon: 0.35079168683259393, Delta: 1e-05\n",
      "Loss: 0.00019720089435577392, Accuracy: 0.2055\n",
      "Epoch 1, loss = 0.008230012841522694\n",
      "Epsilon: 0.6415768935615331, Delta: 1e-05\n",
      "Loss: 5.5314445495605466e-05, Accuracy: 0.7116\n",
      "Epoch 1, loss = 0.04996268078684807\n",
      "Epsilon: 1.0335671024371273, Delta: 1e-05\n",
      "Loss: 0.00015408546924591064, Accuracy: 0.5883\n",
      "Epoch 1, loss = 0.22305183112621307\n",
      "Epsilon: 2.038718812572111, Delta: 1e-05\n",
      "Loss: 0.00021851372718811036, Accuracy: 0.2797\n"
     ]
    }
   ],
   "source": [
    "sample_rates = [0.001, 0.01, 0.03, 0.1]\n",
    "num_epochs=1\n",
    "\n",
    "for sample_rate in sample_rates:\n",
    "    train_loader, test_loader = create_data_loaders(train_tensor, test_tensor, sample_rate)\n",
    "    model = SampleConvNet()\n",
    "    epsilon = train(\n",
    "        model, train_loader, torch.optim.SGD, learning_rate, num_epochs,\n",
    "        noise_multiplier, clip_bound, delta, verbose=True\n",
    "    )\n",
    "    print(\"Epsilon: {}, Delta: {}\".format(epsilon, delta))\n",
    "    average_loss, total_accuracy = test(model, test_loader)\n",
    "    print(\"Loss: {}, Accuracy: {}\".format(average_loss, total_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d286a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.185320347547531e-06, Accuracy: 0.9216\n",
      "Loss: 1.2426486937329174e-07, Accuracy: 0.9194\n",
      "Loss: 2.4844862520694734e-06, Accuracy: 0.9349\n",
      "Loss: 1.8928551580756903e-07, Accuracy: 0.9318\n",
      "Loss: 2.2502657026052476e-06, Accuracy: 0.9118\n",
      "Loss: 1.7072588205337524e-05, Accuracy: 0.8833\n"
     ]
    }
   ],
   "source": [
    "# sample_rate=0.01 looks promising, let's zoom in and look around that\n",
    "sample_rates = [0.006, 0.007, 0.008, 0.009, 0.012, 0.015]\n",
    "for sample_rate in sample_rates:\n",
    "    eps, _, _, acc = pipeline(\n",
    "    learning_rate, noise_multiplier, clip_bound, \n",
    "    sample_rate, num_epochs, data\n",
    "    )\n",
    "    print(\"Epsilon: \", eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4139518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8.183129248209297e-08, Accuracy: 0.9256\n",
      "Epsilon:  11.209556906334038\n",
      "Loss: 3.966782707720995e-07, Accuracy: 0.9338\n",
      "Epsilon:  1.4824070580033397\n",
      "Loss: 1.1773756705224513e-07, Accuracy: 0.9259\n",
      "Epsilon:  0.8361046634978282\n",
      "Loss: 1.3864339562132954e-07, Accuracy: 0.9116\n",
      "Epsilon:  0.4222310450136726\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 0.008\n",
    "\n",
    "noise_multipliers = [0.5, 1., 1.3, 2]\n",
    "for noise_multiplier in noise_multipliers:\n",
    "    eps, _, _, acc = pipeline(\n",
    "    learning_rate, noise_multiplier, clip_bound, \n",
    "    sample_rate, num_epochs, data, verbose=True\n",
    "    )\n",
    "    print(\"Epsilon: \", eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ddbcba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.973552629351616e-07, Accuracy: 0.9285\n",
      "Epsilon:  0.4222310450136726\n"
     ]
    }
   ],
   "source": [
    "# sample_rate=0.01 looks promising, let's zoom in and look around that\n",
    "noise_multiplier = 1.\n",
    "pipeline(\n",
    "    learning_rate, noise_multiplier, clip_bound, \n",
    "    sample_rate, num_epochs, data, verbose=True\n",
    "    )\n",
    "print(\"Epsilon: \", eps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
